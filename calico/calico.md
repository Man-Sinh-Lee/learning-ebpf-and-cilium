Migrating to Cilium

Migrating to Cilium from another CNI is a very common task. But how do we minimize the impact during the migration? How do we ensure pods on the legacy CNI can still communicate to Cilium-managed pods during the migration? How do we execute the migration safely, while avoiding a overly complex approach or using a separe tool such as Multus?

With the use of the new Cilium CRD CiliumNodeConfig, running clusters can be migrated on a node-by-node basis, without disrupting existing traffic or requiring a complete cluster outage or rebuild.

In this lab, you will migrate your cluster from an existing CNI to Cilium. While we use Calico in this lab, you can leverage the same approach for other CNIs (albeit migrating from another CNI might require more efforts).

Note that this feature is still beta and we expect further tooling and automation to be developed to support large cluster migrations.

Let's have a look at this lab's environment.

The cluster has been deployed in the background, with Calico installed on it.

Run the following command to see the Kind cluster configuration:
root@server:~# yq cluster.yaml
---
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    extraPortMappings:
      # nodepinger
      - containerPort: 32042
        hostPort: 32042
      # goldpinger
      - containerPort: 32043
        hostPort: 32043
  - role: worker
  - role: worker
  - role: worker
  - role: worker
networking:
  disableDefaultCNI: true
  podSubnet: 192.168.0.0/16

In the networking section of the configuration file, the default CNI had been disabled. Instead, Calico was deployed to the cluster to provide network connectivity.

To check that the Kind cluster was successfully installed, verify that the nodes are up and joined:
root@server:~# kubectl get nodes
NAME                 STATUS   ROLES           AGE   VERSION
kind-control-plane   Ready    control-plane   14m   v1.29.2
kind-worker          Ready    <none>          14m   v1.29.2
kind-worker2         Ready    <none>          14m   v1.29.2
kind-worker3         Ready    <none>          14m   v1.29.2
kind-worker4         Ready    <none>          14m   v1.29.2

You should see 1 control-plane and 4 nodes appear, all marked as Ready.

Check that Calico was successfully deployed by looking at the status of the Calico DaemonSet:
root@server:~# kubectl -n calico-system rollout status ds/calico-node
daemon set "calico-node" successfully rolled out

Let's check the PodCIDR on each node. (Note: the PodCIDR is the range the Pods will get an IP address from).
root@server:~# kubectl get ipamblocks.crd.projectcalico.org \
  -o jsonpath="{range .items[*]}{'podNetwork: '}{.spec.cidr}{'\t NodeIP: '}{.spec.affinity}{'\n'}{end}"
podNetwork: 192.168.110.128/26   NodeIP: host:kind-worker2
podNetwork: 192.168.162.128/26   NodeIP: host:kind-worker
podNetwork: 192.168.195.192/26   NodeIP: host:kind-worker3
podNetwork: 192.168.218.192/26   NodeIP: host:kind-worker4
podNetwork: 192.168.82.0/26      NodeIP: host:kind-control-plane

We have deployed a software called Goldpinger on the cluster. Goldpinger deploys one pod per node (with a DaemonSet) to monitor node connectivity.

Check that the Nodepinger Daemonset is running properly:
root@server:~# kubectl rollout status daemonset nodepinger-goldpinger
kubectl get po -l app.kubernetes.io/instance=nodepinger -o wide
daemon set "nodepinger-goldpinger" successfully rolled out
NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE                 NOMINATED NODE   READINESS GATES
nodepinger-goldpinger-4sqz8   1/1     Running   0          15m   192.168.82.2      kind-control-plane   <none>           <none>
nodepinger-goldpinger-gpfqv   1/1     Running   0          15m   192.168.218.198   kind-worker4         <none>           <none>
nodepinger-goldpinger-n46mm   1/1     Running   0          15m   192.168.195.193   kind-worker3         <none>           <none>
nodepinger-goldpinger-sx4xc   1/1     Running   0          15m   192.168.110.129   kind-worker2         <none>           <none>
nodepinger-goldpinger-zzfhx   1/1     Running   0          15m   192.168.162.130   kind-worker          <none>           <none>

Go to the 🔗 Nodepinger tab to see the graph generated by these pods.

If necessary, reload the tab by clicking on the reload icon:
We can also check the status in the >_ Terminal tab with curl:
root@server:~# curl -s http://localhost:32042/check | jq
{
  "podResults": {
    "nodepinger-goldpinger-4sqz8": {
      "HostIP": "172.18.0.6",
      "OK": true,
      "PingTime": "2024-08-08T21:16:33.726Z",
      "PodIP": "192.168.82.2",
      "response": {
        "boot_time": "2024-08-08T21:00:15.720Z"
      },
      "status-code": 200
    },
    "nodepinger-goldpinger-gpfqv": {
      "HostIP": "172.18.0.3",
      "OK": true,
      "PingTime": "2024-08-08T21:16:30.673Z",
      "PodIP": "192.168.218.198",
      "response": {
        "boot_time": "2024-08-08T21:00:20.191Z"
      },
      "status-code": 200
    },
    "nodepinger-goldpinger-n46mm": {
      "HostIP": "172.18.0.5",
      "OK": true,
      "PingTime": "2024-08-08T21:16:12.769Z",
      "PodIP": "192.168.195.193",
      "response": {
        "boot_time": "2024-08-08T21:00:15.311Z"
      },
      "status-code": 200
    },
    "nodepinger-goldpinger-sx4xc": {
      "HostIP": "172.18.0.2",
      "OK": true,
      "PingTime": "2024-08-08T21:16:22.978Z",
      "PodIP": "192.168.110.129",
      "response": {
        "boot_time": "2024-08-08T21:00:16.134Z"
      },
      "status-code": 200
    },
    "nodepinger-goldpinger-zzfhx": {
      "HostIP": "172.18.0.4",
      "OK": true,
      "PingTime": "2024-08-08T21:16:24.626Z",
      "PodIP": "192.168.162.130",
      "response": {
        "boot_time": "2024-08-08T21:00:22.704Z"
      },
      "status-code": 200
    }
  }
}

The status code should be 200 for all 5 nodes.

Finally, let's check metrics as it's easier to see how many nodes are healthy or unhealthy:
root@server:~# curl -s http://localhost:32042/metrics | grep '^goldpinger_nodes_health_total'
goldpinger_nodes_health_total{goldpinger_instance="kind-worker4",status="healthy"} 5
goldpinger_nodes_health_total{goldpinger_instance="kind-worker4",status="unhealthy"} 0

The first Goldpinger deployment (which we called "Nodepinger") is practical, but since it uses DaemonSets, these pods won't get deleted when we drain nodes for the migration.

To show the difference, we'll also deploy Goldpinger a second time as a Deployment with 10 replicas:
kubectl apply -f /tmp/goldpinger_deploy.yaml
root@server:~# kubectl rollout status deployment goldpinger
kubectl get po -l app=goldpinger -o wide
deployment "goldpinger" successfully rolled out
NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE                 NOMINATED NODE   READINESS GATES
goldpinger-57dfcb7b86-8jfps   1/1     Running   0          85s   192.168.110.131   kind-worker2         <none>           <none>
goldpinger-57dfcb7b86-9r24f   1/1     Running   0          85s   192.168.162.133   kind-worker          <none>           <none>
goldpinger-57dfcb7b86-jshrm   1/1     Running   0          85s   192.168.162.132   kind-worker          <none>           <none>
goldpinger-57dfcb7b86-jx267   1/1     Running   0          85s   192.168.195.196   kind-worker3         <none>           <none>
goldpinger-57dfcb7b86-l4p88   1/1     Running   0          85s   192.168.110.132   kind-worker2         <none>           <none>
goldpinger-57dfcb7b86-s46j9   1/1     Running   0          85s   192.168.82.3      kind-control-plane   <none>           <none>
goldpinger-57dfcb7b86-t64xn   1/1     Running   0          85s   192.168.82.4      kind-control-plane   <none>           <none>
goldpinger-57dfcb7b86-v76jn   1/1     Running   0          85s   192.168.218.199   kind-worker4         <none>           <none>
goldpinger-57dfcb7b86-w4l6c   1/1     Running   0          85s   192.168.195.197   kind-worker3         <none>           <none>
goldpinger-57dfcb7b86-zhnct   1/1     Running   0          85s   192.168.218.200   kind-worker4         <none>           <none>

Then expose the pods as a service on NodePort 32043:
root@server:~# kubectl expose deployment goldpinger --type NodePort \
  --overrides '{"spec":{"ports": [{"port":80,"protocol":"TCP","targetPort":8080,"nodePort":32043}]}}'
service/goldpinger exposed

Check the status of these pods by looking at the 🔗 Goldpinger tab.

Refresh the tab if necessary by clicking on the reload icon in the top right corner.

Back in the >_ Terminal, you can also check the status with curl:
root@server:~# curl -s http://localhost:32043/metrics | grep '^goldpinger_nodes_health_total'
goldpinger_nodes_health_total{goldpinger_instance="kind-worker3",status="healthy"} 10
goldpinger_nodes_health_total{goldpinger_instance="kind-worker3",status="unhealthy"} 0

You should see 10 healthy and 0 unhealthy pods (if it doesn't, try again).

In the next tasks, we will prepare the cluster for the migration from Calico to Cilium.


Kubelet and CNI plugins

On every node, the Kubelet is configured (via files in the /etc/cni/net.d/ directory) to use one (or several) CNI plugins.

The CNI plugins handle networking tasks for Pods:

    allocating IP address(es)
    creating & configuring the Pod's network interface(s)
    (potentially) establishing an overlay network

The Pod’s network configuration shares the same life cycle as the Pod Sandbox.

When migrating CNI plugins, several approaches are possible, with pros and cons.
Migration Approaches

    The ideal scenario is to build a brand new cluster and migrate workloads using a GitOps approach. This can however involve a lot of preparation work and potential disruptions.
    Another method consists in reconfiguring /etc/cni/net.d/ to point to Cilium. However, existing Pods will still have been configured by the old network plugin while new Pods will be configured by the newer CNI plugin. To complete the migration, all Pods on the cluster that are configured by the old CNI must be recycled in order to be managed by the new CNI plugin.
    A naive approach to migrating a CNI would be to reconfigure all nodes with a new CNI and then gradually restart each node in the cluster, thus replacing the CNI when the node is brought back up and ensuring that all pods are part of the new CNI. This simple migration, while effective, comes at the cost of disrupting cluster connectivity during the rollout. Unmigrated and migrated nodes would be split in to two “islands” of connectivity, and pods would be randomly unable to reach one-another until the migration is complete.

In this lab, you will learn about a new approach
Migration via dual overlays

Cilium supports a hybrid mode, where two separate overlays are established across the cluster. While Pods on a given node can only be attached to one network, they have access to both Cilium and non-Cilium pods while the migration is taking place. As long as Cilium and the existing networking provider use a separate IP range, the Linux routing table takes care of separating traffic.

In this lab, we will use a model for live migrating between two deployed CNI implementations. This will have the benefit of reducing downtime of nodes and workloads and ensuring that workloads on both configured CNIs can communicate during migration.

For live migration to work, Cilium will be installed with a separate CIDR range and encapsulation port than that of the currently installed CNI. As long as Cilium and the existing CNI use a separate IP range, the Linux routing table takes care of separating traffic

Requirements

Live migration requires the following:

    An existing CNI plugin that uses the Linux routing stack, such as Flannel, Calico, or AWS-CNI
    A Cilium Pod CIDR distinct from the previous CNI plugin
    A Cilium overlay distinct from the previous CNI plugin, either by changing the protocol or port
    Use of the Cilium Cluster Pool IPAM mode


Migration Overview

The migration process utilizes the per-node configuration feature to selectively enable Cilium CNI. This allows for a controlled rollout of Cilium without disrupting existing workloads.

Cilium will be installed, first, in a mode where it establishes an overlay but does not provide CNI networking for any pods. Then, individual nodes will be migrated.

In summary, the process looks like:

    Prepare the cluster and install Cilium in “secondary” mode.
    Cordon, drain, migrate, and reboot each node
    Remove the existing network provider
    (Optional) Reboot each node again

In the next task, you will prepare the cluster and install Cilium in "secondary" mode.

The first step is to select a new CIDR for pods. It must be distinct from all other CIDRs in use.

Let's check the CIDR used by Calico:
root@server:~# kubectl get installations.operator.tigera.io default \
  -o jsonpath='{.spec.calicoNetwork.ipPools[*].cidr}{"\n"}'
192.168.0.0/16

Calico is using its default CIDR, which is 192.168.0.0/16. In order to avoid conflicts, we will use 10.244.0.0/16 —which is the usual default on Kind— as the pod CIDR for Cilium.

The second step is to select a different encapsulation protocol (Geneve instead of VXLAN for example) or a distinct encapsulation port.

Check which encapsulation protocol Calico is using:
root@server:~# kubectl get installations.operator.tigera.io default \
  -o jsonpath='{.spec.calicoNetwork.ipPools[*].encapsulation}{"\n"}'
VXLANCrossSubnet

Calico is using VXLANCrossSubnet. In order to avoid clashing with Calico's VXLAN port, we need to use VXLAN with a non-default port. Since the standard port is 8472, let's use 8473 instead.

We have pre-created a Cilium Helm configuration file values-migration.yaml based on the details above:

Review it:
root@server:~# yq values-migration.yaml
---
operator:
  unmanagedPodWatcher:
    restart: false
tunnel: vxlan
tunnelPort: 8473
cni:
  customConf: true
  uninstall: false
ipam:
  mode: "cluster-pool"
  operator:
    clusterPoolIPv4PodCIDRList: ["10.244.0.0/16"]
policyEnforcementMode: "never"
bpf:
  hostLegacyRouting: true

 Operator Configuration
yaml

operator:
  unmanagedPodWatcher:
    restart: false

This is there to prevent the Cilium Operator from restarting Pods that are not being managed by Cilium (we don't want to disrupt the pods that are not managed by Cilium).
🔌 VXLAN Configuration
yaml

tunnelPort: 8473

As highlighted earlier, this is to prevent Cilium's VXLAN from conflicting with Calico's.
🌐 CNI Configuration
yaml

cni:
  customConf: true
  uninstall: false

The first setting (customConf: true) above temporarily skips writing the CNI configuration. This is to prevent Cilium from taking over immediately.

Once the migration is complete, we will switch customConf back to the default false value.

The second setting (uninstall: false) above will prevent Cilium from removing the CNI configuration file and plugin binaries for Calico, thus allowing the temporary migration state.
🪪 IPAM Configuration
yaml

ipam:
  mode: "cluster-pool"
  operator:
    clusterPoolIPv4PodCIDRList: ["10.244.0.0/16"]

Live migration requires to use the Cluster Pool IPAM mode, and a Pod CIDR distinct from Calico's in order to perform the migration.
🛡️ Network Policies Configuration
yaml

policyEnforcementMode: "never"

The above disables the enforcement of network policy until the migration is completed. We will enforce network policies post-migration.
🐝 BPF Configuration
yaml

bpf:
  hostLegacyRouting: true

This flag routes traffic via the host stack to provide connectivity during the migration. We will verify during the migration that Calico-managed pods and Cilium-managed pods have connectivity.  

In this lab, we will use cilium-cli to auto-detect settings specific to the underlying cluster platform (Kind in this case) and generate Helm values. We will then use the Helm chart to install Cilium.

Let's generate the values-migration.yaml Helm values file:
root@server:~# cilium install \
  --helm-values values-migration.yaml \
  --dry-run-helm-values > values-initial.yaml

The above command:

    uses Helm values from values-migration.yaml
    automatically fills in the missing values through the use of the helm-auto-gen-values flag
    creates a new Helm values file called values-initial.yaml

Review the created file:
root@server:~# yq values-initial.yaml
bpf:
  hostLegacyRouting: true
cluster:
  name: kind-kind
cni:
  customConf: true
  uninstall: false
ipam:
  mode: cluster-pool
  operator:
    clusterPoolIPv4PodCIDRList:
      - 10.244.0.0/16
operator:
  replicas: 1
  unmanagedPodWatcher:
    restart: false
policyEnforcementMode: never
routingMode: tunnel
tunnel: vxlan
tunnelPort: 8473
tunnelProtocol: vxlan

It is a combination of the values pulled from the values-migration.yaml file and the one auto-generated by the Cilium CLI, such as:

    operator.replicas
    cluster.id and cluster.name
    kubeProxyReplacement
    the serviceAccounts section
    encryption.nodeEncryption

Calico has several modes of operation to decide which network interface to use by default on nodes that it manages. This is configured using the Tigera Operator's spec.calicoNetwork.nodeAddressAutodetectionV4 (and respectively nodeAddressAutodetectionV6 for IPv6) parameter.

By default, it is set to firstFound: true, which will use the first detected network interface on the node. Check this value with:
root@server:~# kubectl get installations.operator.tigera.io default \
  -o jsonpath='{.spec.calicoNetwork.nodeAddressAutodetectionV4}{"\n"}'
{"firstFound":true}

When installing Cilium on the nodes, Cilium will create a new network interface called cilium_host. If Calico decides to use it as its default interface, Calico node routing will start failing. For this reason, we want to make sure that Calico ignores the cilium_host interface.

Depending on your Tigera Operator settings (for example if you use the interface or skipInterface options), you might want to adjust the parameters to ensure cilium_host is not considered.

In our lab, we will simply set firstFound to false and use kubernetes: NodeInternalIP instead, so Calico uses the node's internal IP as its main interface. Patch the Tigera Operator's configuration with:
root@server:~# kubectl patch installations.operator.tigera.io default --type=merge \
  --patch '{"spec": {"calicoNetwork": {"nodeAddressAutodetectionV4": {"firstFound": false, "kubernetes": "NodeInternalIP"}}}}'
installation.operator.tigera.io/default patched

root@server:~# kubectl get installations.operator.tigera.io default \
  -o jsonpath='{.spec.calicoNetwork.nodeAddressAutodetectionV4}{"\n"}'
{"firstFound":false,"kubernetes":"NodeInternalIP"}

Let's now install Cilium using helm and the values we have just generated.
root@server:~# helm repo add cilium https://helm.cilium.io/
helm upgrade --install --namespace kube-system cilium cilium/cilium \
  --values values-initial.yaml
"cilium" has been added to your repositories
Release "cilium" does not exist. Installing it now.
NAME: cilium
LAST DEPLOYED: Thu Aug  8 21:34:37 2024
NAMESPACE: kube-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble.

Your release version is 1.16.0.

For any further help, visit https://docs.cilium.io/en/v1.16/gettinghelp
root@server:~# cilium status --wait
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium-envoy       Desired: 5, Ready: 5/5, Available: 5/5
Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium             Desired: 5, Ready: 5/5, Available: 5/5
Containers:            cilium             Running: 5
                       cilium-envoy       Running: 5
                       cilium-operator    Running: 1
Cluster Pods:          0/26 managed by Cilium
Helm chart version:    
Image versions         cilium             quay.io/cilium/cilium:v1.16.0@sha256:46ffa4ef3cf6d8885dcc4af5963b0683f7d59daa90d49ed9fb68d3b1627fe058: 5
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51@sha256:bd5ff8c66716080028f414ec1cb4f7dc66f40d2fb5a009fff187f4a9b90b566b: 5
                       cilium-operator    quay.io/cilium/operator-generic:v1.16.0@sha256:d6621c11c4e4943bf2998af7febe05be5ed6fdcf812b27ad4388f47022190316: 1

Note that the Cluster Pods entry indicates that no pods are managed by Cilium. While Cilium is installed on every node and an overlay is established between the nodes, it is not yet configured to manage pods on nodes.

Check the CNI configuration on one of the nodes:
root@server:~# docker exec kind-worker ls /etc/cni/net.d/
10-calico.conflist
calico-kubeconfig
It still only contains Calico configuration, so Kubelets are unable to make use of Cilium for now.

In a standard installation, all Cilium agents have the same configuration, controlled by the cilium-config ConfigMap resource.

However in our migration, we want to rollout Cilium on one node at a time. In order to achieve this, we will use the CiliumNodeConfig resource type (a CRD that was added in Cilium 1.13), which allows to configure Cilium agents on a per-node basis.

A CiliumNodeConfig object consists of a set of fields and a label selector. The label selector defines to which nodes the configuration applies.

Let's now create a per-node config that will instruct Cilium to “take over” CNI networking on the node.

root@server:~# yq ciliumnodeconfig.yaml 
apiVersion: cilium.io/v2alpha1
kind: CiliumNodeConfig
metadata:
  namespace: kube-system
  name: cilium-default
spec:
  nodeSelector:
    matchLabels:
      io.cilium.migration/cilium-default: "true"
  defaults:
    write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist
    custom-cni-conf: "false"
    cni-chaining-mode: "none"
    cni-exclusive: "true"

This configuration will be used to switch the nodes' CNI configuration from Calico to Cilium.

When the file is saved, head back to the >_ Terminal tab and apply it:
root@server:~# kubectl apply --server-side -f ciliumnodeconfig.yaml
ciliumnodeconfig.cilium.io/cilium-default serverside-applied

Check the list of nodes on the cluster, with their labels:
root@server:~# kubectl get no --show-labels
NAME                 STATUS   ROLES           AGE   VERSION   LABELS
kind-control-plane   Ready    control-plane   41m   v1.29.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kind-control-plane,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
kind-worker          Ready    <none>          41m   v1.29.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kind-worker,kubernetes.io/os=linux
kind-worker2         Ready    <none>          41m   v1.29.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kind-worker2,kubernetes.io/os=linux
kind-worker3         Ready    <none>          41m   v1.29.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kind-worker3,kubernetes.io/os=linux
kind-worker4         Ready    <none>          41m   v1.29.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kind-worker4,kubernetes.io/os=linux

There is currently no nodes matching the io.cilium.migration/cilium-default: "true" condition in our CiliumNodeConfig resource. For this reason, this configuration does not apply to any nodes for now.

In the next challenge, we will start migrating nodes by applying this label to them, one by one.

Migration

We are now ready to begin the migration process. We will do it a node at a time.

We will cordon, drain, migrate, and reboot each node.


Check the 🔗 Goldpinger tab to verify that the pods are still able to communicate with each other fine.
It is recommended to always cordon and drain at the beginning of the migration process, so that end-users are not impacted by any potential issues.

Cordoning a node will prevent new pods from being scheduled on it.

Draining a node will gracefully evict all the running pods from the node. This ensures that the pods are not abruptly terminated and that their workload is gracefully handled by other available nodes.

Navigate back to the >_ Terminal tab.

Let's get started with the kind-worker node:

root@server:~# NODE="kind-worker"
root@server:~# kubectl cordon $NODE
node/kind-worker cordoned
root@server:~# 

Expect to see an output such as node/kind-worker cordoned.

Let's now drain the node. Note that we use the ignore-daemonset flag as several DaemonSets are still required to run. When we drain a node, the node is automatically cordoned. We cordoned first in this instance to provide clarity in the migration process, but you don't need to do both steps in the future.

root@server:~# kubectl drain $NODE --ignore-daemonsets
node/kind-worker already cordoned
Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-tvnrj, calico-system/csi-node-driver-5l9sj, default/nodepinger-goldpinger-zzfhx, kube-system/cilium-envoy-rsrkh, kube-system/cilium-hwnwg, kube-system/kube-proxy-jkx2t
evicting pod default/goldpinger-57dfcb7b86-jshrm
evicting pod calico-apiserver/calico-apiserver-77c8ccd67c-q4g94
evicting pod default/goldpinger-57dfcb7b86-9r24f
evicting pod calico-system/calico-typha-bf9946fc4-xszl4
pod/goldpinger-57dfcb7b86-jshrm evicted
pod/calico-apiserver-77c8ccd67c-q4g94 evicted
pod/goldpinger-57dfcb7b86-9r24f evicted
pod/calico-typha-bf9946fc4-xszl4 evicted
node/kind-worker drained
root@server:~# 



Let's verify no pods are running on the drained node (besides the goldpinger pod which is part of a DaemonSet):
root@server:~# kubectl get pods -o wide --field-selector spec.nodeName=$NODE
NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE          NOMINATED NODE   READINESS GATES
nodepinger-goldpinger-zzfhx   1/1     Running   0          44m   192.168.162.130   kind-worker   <none>           <none>

Verify that the Nodepinger still sees 5 pods:
root@server:~# curl -s http://localhost:32042/metrics | grep '^goldpinger_nodes_health_total'
goldpinger_nodes_health_total{goldpinger_instance="kind-worker4",status="healthy"} 5
goldpinger_nodes_health_total{goldpinger_instance="kind-worker4",status="unhealthy"} 0

We can now label the node, which will cause the CiliumNodeConfig to apply to this node.
root@server:~# kubectl label node $NODE --overwrite "io.cilium.migration/cilium-default=true"
node/kind-worker labeled
Restart Cilium on the node. That will trigger the creation of CNI configuration file.
root@server:~# kubectl -n kube-system delete pod --field-selector spec.nodeName=$NODE -l k8s-app=cilium
kubectl -n kube-system rollout status ds/cilium -w
pod "cilium-hwnwg" deleted
Waiting for daemon set "cilium" rollout to finish: 4 of 5 updated pods are available...
daemon set "cilium" successfully rolled out

Check the CNI configurations on the node:
root@server:~# docker exec $NODE ls /etc/cni/net.d/
05-cilium.conflist
10-calico.conflist.cilium_bak
calico-kubeconfig

Cilium has renamed the Calico configuration (10-calico.conflist) and deployed its own configuration file 05-cilium.conflist, so the Kubelet on that node is now configured to use Cilium as its CNI provider.

Check the Nodepinger pods:
root@server:~# kubectl get po -l app.kubernetes.io/instance=nodepinger \
  --field-selector spec.nodeName=$NODE -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP                NODE          NOMINATED NODE   READINESS GATES
nodepinger-goldpinger-zzfhx   1/1     Running   0          46m   192.168.162.130   kind-worker   <none>           <none>

It is still using the Calico pod CIDR, since the pod was not restarted yet. Delete it to force it to restart:
root@server:~# kubectl delete po -l app.kubernetes.io/instance=nodepinger \
  --field-selector spec.nodeName=$NODE
pod "nodepinger-goldpinger-zzfhx" deleted
root@server:~# kubectl get po -l app.kubernetes.io/instance=nodepinger \
  --field-selector spec.nodeName=$NODE -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
nodepinger-goldpinger-nfqjr   1/1     Running   0          5s    10.244.1.148   kind-worker   <none>           <none>

It has now received an IP from the Cilium pod CIDR range (10.244.0.0/16).

Verify that the connectivity is still fine:
root@server:~# curl -s http://localhost:32042/metrics | grep '^goldpinger_nodes_health_total'
goldpinger_nodes_health_total{goldpinger_instance="kind-worker",status="healthy"} 4
goldpinger_nodes_health_total{goldpinger_instance="kind-worker",status="unhealthy"} 0


Let's check the Cilium status. It may take a minute or so.
root@server:~# cilium status --wait
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium             Desired: 5, Ready: 5/5, Available: 5/5
DaemonSet              cilium-envoy       Desired: 5, Ready: 5/5, Available: 5/5
Containers:            cilium-operator    Running: 1
                       cilium-envoy       Running: 5
                       cilium             Running: 5
Cluster Pods:          1/26 managed by Cilium
Helm chart version:    
Image versions         cilium             quay.io/cilium/cilium:v1.16.0@sha256:46ffa4ef3cf6d8885dcc4af5963b0683f7d59daa90d49ed9fb68d3b1627fe058: 5
                       cilium-operator    quay.io/cilium/operator-generic:v1.16.0@sha256:d6621c11c4e4943bf2998af7febe05be5ed6fdcf812b27ad4388f47022190316: 1
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51@sha256:bd5ff8c66716080028f414ec1cb4f7dc66f40d2fb5a009fff187f4a9b90b566b: 5

The Cluster Pods section indicates that 1 Pod is managed by Cilium, the goldpinger Pod (since it is part of a DaemonSet).

Check the Pod CIDR that was assigned to the node by Cilium:                      
root@server:~# kubectl get ciliumnode kind-worker \
  -o jsonpath='{.spec.ipam.podCIDRs[0]}{"\n"}'
10.244.1.0/24

root@server:~# kubectl uncordon $NODE
node/kind-worker uncordoned
root@server:~# kubectl scale deployment goldpinger --replicas 15
deployment.apps/goldpinger scaled
root@server:~# kubectl get po -l app=goldpinger --field-selector spec.nodeName=$NODE -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
goldpinger-57dfcb7b86-7lpws   1/1     Running   0          6s    10.244.1.106   kind-worker   <none>           <none>
goldpinger-57dfcb7b86-dq5ql   1/1     Running   0          6s    10.244.1.26    kind-worker   <none>           <none>
goldpinger-57dfcb7b86-p9h57   1/1     Running   0          6s    10.244.1.1     kind-worker   <none>           <none>


Then check that all pings work fine with the previously deployed pods in the deployment (it might take a little while, relaunch the command):
root@server:~# curl -s http://localhost:32043/metrics | grep '^goldpinger_nodes_health_total'
goldpinger_nodes_health_total{goldpinger_instance="kind-worker4",status="healthy"} 15
goldpinger_nodes_health_total{goldpinger_instance="kind-worker4",status="unhealthy"} 0

You can also check the 🔗 Goldpinger tab and refresh it to check that all 15 pods can talk to each other.

Let's migrate all the other worker nodes, by looping on all of them:
for i in $(seq 2 4); do
  node="kind-worker${i}"
  echo "Migrating node ${node}"
  kubectl drain $node --ignore-daemonsets
  kubectl label node $node --overwrite "io.cilium.migration/cilium-default=true"
  kubectl -n kube-system delete pod --field-selector spec.nodeName=$node -l k8s-app=cilium
  kubectl -n kube-system rollout status ds/cilium -w
  kubectl uncordon $node
done

Check that the Cilium status is correct:
root@server:~# cilium status --wait
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium             Desired: 5, Ready: 5/5, Available: 5/5
Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium-envoy       Desired: 5, Ready: 5/5, Available: 5/5
Containers:            cilium             Running: 5
                       cilium-operator    Running: 1
                       cilium-envoy       Running: 5
Cluster Pods:          19/31 managed by Cilium
Helm chart version:    
Image versions         cilium             quay.io/cilium/cilium:v1.16.0@sha256:46ffa4ef3cf6d8885dcc4af5963b0683f7d59daa90d49ed9fb68d3b1627fe058: 5
                       cilium-operator    quay.io/cilium/operator-generic:v1.16.0@sha256:d6621c11c4e4943bf2998af7febe05be5ed6fdcf812b27ad4388f47022190316: 1
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51@sha256:bd5ff8c66716080028f414ec1cb4f7dc66f40d2fb5a009fff187f4a9b90b566b: 5

The Nodepinger DaemonSet pods will still be on the old Pod CIDR, so let's restart all of them:

root@server:~# kubectl rollout restart daemonset nodepinger-goldpinger
kubectl rollout status daemonset nodepinger-goldpinger
daemonset.apps/nodepinger-goldpinger restarted
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 0 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 2 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 2 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 3 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 3 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 of 5 updated pods are available...
daemon set "nodepinger-goldpinger" successfully rolled out

root@server:~# kubectl get po -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP             NODE                 NOMINATED NODE   READINESS GATES
goldpinger-57dfcb7b86-7lpws   1/1     Running   0          18m     10.244.1.106   kind-worker          <none>           <none>
goldpinger-57dfcb7b86-8cbqp   1/1     Running   0          2m13s   10.244.3.218   kind-worker2         <none>           <none>
goldpinger-57dfcb7b86-8z6ct   1/1     Running   0          2m33s   10.244.1.14    kind-worker          <none>           <none>
goldpinger-57dfcb7b86-bhmm5   1/1     Running   0          2m13s   10.244.3.108   kind-worker2         <none>           <none>
goldpinger-57dfcb7b86-dq5ql   1/1     Running   0          18m     10.244.1.26    kind-worker          <none>           <none>
goldpinger-57dfcb7b86-h7jqx   1/1     Running   0          98s     10.244.4.138   kind-worker3         <none>           <none>
goldpinger-57dfcb7b86-hkbc2   1/1     Running   0          18m     192.168.82.5   kind-control-plane   <none>           <none>
goldpinger-57dfcb7b86-hrf9r   1/1     Running   0          98s     10.244.4.218   kind-worker3         <none>           <none>
goldpinger-57dfcb7b86-nlctj   1/1     Running   0          98s     10.244.4.10    kind-worker3         <none>           <none>
goldpinger-57dfcb7b86-p9h57   1/1     Running   0          18m     10.244.1.1     kind-worker          <none>           <none>
goldpinger-57dfcb7b86-qjbhn   1/1     Running   0          98s     10.244.4.200   kind-worker3         <none>           <none>
goldpinger-57dfcb7b86-qljk8   1/1     Running   0          2m13s   10.244.3.210   kind-worker2         <none>           <none>
goldpinger-57dfcb7b86-qr44n   1/1     Running   0          2m13s   10.244.3.91    kind-worker2         <none>           <none>
goldpinger-57dfcb7b86-s46j9   1/1     Running   0          50m     192.168.82.3   kind-control-plane   <none>           <none>
goldpinger-57dfcb7b86-t64xn   1/1     Running   0          50m     192.168.82.4   kind-control-plane   <none>           <none>
nodepinger-goldpinger-89vxf   1/1     Running   0          20s     10.244.3.229   kind-worker2         <none>           <none>
nodepinger-goldpinger-cfpw4   1/1     Running   0          22s     192.168.82.6   kind-control-plane   <none>           <none>
nodepinger-goldpinger-n2mjc   1/1     Running   0          19s     10.244.1.153   kind-worker          <none>           <none>
nodepinger-goldpinger-rnd6b   1/1     Running   0          24s     10.244.4.73    kind-worker3         <none>           <none>
nodepinger-goldpinger-sj92b   1/1     Running   0          25s     10.244.2.189   kind-worker4         <none>           <none>

Check that connectivity is still fine for both the DaemonSet and the Deployment Goldpinger apps:
root@server:~# curl -s http://localhost:32042/metrics | grep '^goldpinger_nodes_health_total'
curl -s http://localhost:32043/metrics | grep '^goldpinger_nodes_health_total'
goldpinger_nodes_health_total{goldpinger_instance="kind-control-plane",status="healthy"} 3
goldpinger_nodes_health_total{goldpinger_instance="kind-control-plane",status="unhealthy"} 1
goldpinger_nodes_health_total{goldpinger_instance="kind-worker2",status="healthy"} 15
goldpinger_nodes_health_total{goldpinger_instance="kind-worker2",status="unhealthy"} 0


The only node left to migrate is the control plane.
We can now proceed to the migration of the final node.

Let's cordon the node:
root@server:~# NODE="kind-control-plane"
kubectl drain $NODE --ignore-daemonsets
kubectl label node $NODE --overwrite "io.cilium.migration/cilium-default=true"
kubectl -n kube-system delete pod --field-selector spec.nodeName=$NODE -l k8s-app=cilium
kubectl -n kube-system rollout status ds/cilium -w
kubectl uncordon $NODE
node/kind-control-plane cordoned
Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-85d7n, calico-system/csi-node-driver-5wnn8, default/nodepinger-goldpinger-cfpw4, kube-system/cilium-envoy-pwl8s, kube-system/cilium-fj5kf, kube-system/kube-proxy-2ns8b
evicting pod default/goldpinger-57dfcb7b86-t64xn
evicting pod default/goldpinger-57dfcb7b86-hkbc2
evicting pod default/goldpinger-57dfcb7b86-s46j9
pod/goldpinger-57dfcb7b86-hkbc2 evicted
pod/goldpinger-57dfcb7b86-s46j9 evicted
pod/goldpinger-57dfcb7b86-t64xn evicted
node/kind-control-plane drained
node/kind-control-plane labeled
pod "cilium-fj5kf" deleted
Waiting for daemon set "cilium" rollout to finish: 4 of 5 updated pods are available...
daemon set "cilium" successfully rolled out
node/kind-control-plane uncordoned
root@server:~# kubectl rollout restart daemonset nodepinger-goldpinger
kubectl rollout status daemonset nodepinger-goldpinger
daemonset.apps/nodepinger-goldpinger restarted
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 0 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 2 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 2 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 3 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 3 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "nodepinger-goldpinger" rollout to finish: 4 of 5 updated pods are available...
daemon set "nodepinger-goldpinger" successfully rolled out

In addition, restart all csi-node-driver DaemonSet pods inside the calico-system namespace to complete the migration of all existing pods to Cilium:
root@server:~# kubectl rollout restart daemonset -n calico-system csi-node-driver
kubectl rollout status daemonset -n calico-system csi-node-driver
daemonset.apps/csi-node-driver restarted
Waiting for daemon set "csi-node-driver" rollout to finish: 0 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 1 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 2 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 2 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 3 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 3 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 4 out of 5 new pods have been updated...
Waiting for daemon set "csi-node-driver" rollout to finish: 4 of 5 updated pods are available...
daemon set "csi-node-driver" successfully rolled out


The status of Cilium should be OK and all pods should be managed by Cilium:
root@server:~# cilium status --wait
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium-envoy       Desired: 5, Ready: 5/5, Available: 5/5
DaemonSet              cilium             Desired: 5, Ready: 5/5, Available: 5/5
Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium             Running: 5
                       cilium-operator    Running: 1
                       cilium-envoy       Running: 5
Cluster Pods:          31/31 managed by Cilium
Helm chart version:    
Image versions         cilium             quay.io/cilium/cilium:v1.16.0@sha256:46ffa4ef3cf6d8885dcc4af5963b0683f7d59daa90d49ed9fb68d3b1627fe058: 5
                       cilium-operator    quay.io/cilium/operator-generic:v1.16.0@sha256:d6621c11c4e4943bf2998af7febe05be5ed6fdcf812b27ad4388f47022190316: 1
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51@sha256:bd5ff8c66716080028f414ec1cb4f7dc66f40d2fb5a009fff187f4a9b90b566b: 5

Clean-up

Now the migration has been completed, let's update the Cilium configuration to support Network Policies and remove the previous network plugin.

Now that Cilium is healthy, let's update the Cilium configuration.

First, generate a new Helm values file by overriding some parameters:

root@server:~# cilium install \
  --helm-values values-initial.yaml \
  --helm-set operator.unmanagedPodWatcher.restart=true \
  --helm-set cni.customConf=false \
  --helm-set policyEnforcementMode=default \
  --dry-run-helm-values > values-final.yaml

Again, we are using the cilium-cli to generate an updated Helm config file. Check the differences with the previous values:
root@server:~# diff -u --color values-initial.yaml values-final.yaml
--- values-initial.yaml 2024-08-08 21:29:17.871525659 +0000
+++ values-final.yaml   2024-08-08 22:13:45.129912254 +0000
@@ -3,7 +3,7 @@
 cluster:
   name: kind-kind
 cni:
-  customConf: true
+  customConf: false
   uninstall: false
 ipam:
   mode: cluster-pool
@@ -13,8 +13,8 @@
 operator:
   replicas: 1
   unmanagedPodWatcher:
-    restart: false
-policyEnforcementMode: never
+    restart: true
+policyEnforcementMode: default
 routingMode: tunnel
 tunnel: vxlan
 tunnelPort: 8473

As you can see from checking the differences between the two files, we are changing three parameters:

    enabling Cilium to write the CNI configuration file by disabling a per node configuration (customConf)
    enabling Cilium to restart unmanaged pods
    enabling Network Policy Enforcement

Let's apply it and rollout the Cilium DaemonSet:
root@server:~# helm upgrade --install \
  --namespace kube-system cilium cilium/cilium \
  --values values-final.yaml
kubectl -n kube-system rollout restart daemonset cilium
cilium status --wait
Release "cilium" has been upgraded. Happy Helming!
NAME: cilium
LAST DEPLOYED: Thu Aug  8 22:14:56 2024
NAMESPACE: kube-system
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble.

Your release version is 1.16.0.

For any further help, visit https://docs.cilium.io/en/v1.16/gettinghelp
daemonset.apps/cilium restarted
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium-envoy       Desired: 5, Ready: 5/5, Available: 5/5
DaemonSet              cilium             Desired: 5, Ready: 5/5, Available: 5/5
Containers:            cilium             Running: 5
                       cilium-operator    Running: 1
                       cilium-envoy       Running: 5
Cluster Pods:          31/31 managed by Cilium
Helm chart version:    
Image versions         cilium-operator    quay.io/cilium/operator-generic:v1.16.0@sha256:d6621c11c4e4943bf2998af7febe05be5ed6fdcf812b27ad4388f47022190316: 1
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51@sha256:bd5ff8c66716080028f414ec1cb4f7dc66f40d2fb5a009fff187f4a9b90b566b: 5
                       cilium             quay.io/cilium/cilium:v1.16.0@sha256:46ffa4ef3cf6d8885dcc4af5963b0683f7d59daa90d49ed9fb68d3b1627fe058: 5
root@server:~# 

Remove the CiliumNodeConfig resource:
root@server:~# kubectl delete -n kube-system ciliumnodeconfig cilium-default
ciliumnodeconfig.cilium.io "cilium-default" deleted

Let's remove Calico as it is no longer needed:
kubectl delete --force -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/tigera-operator.yaml
kubectl delete --force -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/custom-resources.yaml
kubectl delete --force namespace calico-system

Finally, let's restart the nodes one by one:
for i in " " $(seq 1 4); do
  node="kind-worker${i}"
  echo "Restarting node ${node}"
  docker restart $node
  sleep 5 # wait for cilium to catch that the node is missing
  kubectl -n kube-system rollout status ds/cilium -w
done
Let's finish with the control plane node:
docker restart kind-control-plane
sleep 5
kubectl -n kube-system rollout status ds/cilium -w

Check Cilium status:
cilium status --wait

All pods should now be managed by Cilium (see the "Cluster Pods" section).

Check that connectivity is still fine for both the DaemonSet and the Deployment Goldpinger apps:
curl -s http://localhost:32042/metrics | grep '^goldpinger_nodes_health_total'
curl -s http://localhost:32043/metrics | grep '^goldpinger_nodes_health_total'



Since we've restarted the only control plane in this cluster (you should have at least 3 in production clusters), the state might be a bit broken for a little while.

Check the connectivity:
cilium connectivity test